{"cells":[{"cell_type":"markdown","metadata":{"id":"V9zNGvape2-I"},"source":["# **Augmentor**\n","\n","<font size = 4>Data augmentation can improve training progress by amplifying differences in the dataset. This can be useful if the available dataset is small since, in this case, it is possible that a network could quickly learn every example in the dataset (overfitting), without augmentation. Augmentation is not necessary for training and if your training dataset is large you should disable it.\n","\n","\n","---\n","\n","<font size = 4>*Disclaimer*:\n","\n","<font size = 4>This notebook is part of the *Zero-Cost Deep-Learning to Enhance Microscopy* project (https://github.com/HenriquesLab/DeepLearning_Collab/wiki). Jointly developed by the Jacquemet (link to https://cellmig.org/) and Henriques (https://henriqueslab.github.io/) laboratories.\n","\n","<font size = 4>[Augmentor](https://github.com/mdbloice/Augmentor) was described in the following article:\n","\n","<font size = 4>Marcus D Bloice, Peter M Roth, Andreas Holzinger, Biomedical image augmentation using Augmentor, Bioinformatics, https://doi.org/10.1093/bioinformatics/btz259\n","\n","<font size = 4>**Please also cite this original paper when using or developing this notebook.**"]},{"cell_type":"markdown","metadata":{"id":"n4yWFoJNnoin"},"source":["# **2. Install Augmentor and Dependencies**\n","---\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"3u2mXn3XsWzd"},"outputs":[],"source":["Notebook_version = '1.13'\n","Network = 'Augmentor'\n","\n","import Augmentor\n","import os\n","\n","# ------- Common variable to all ZeroCostDL4Mic notebooks -------\n","import numpy as np\n","from matplotlib import pyplot as plt\n","import urllib\n","import os, random\n","import shutil \n","import zipfile\n","from tifffile import imread, imsave\n","import time\n","import sys\n","from pathlib import Path\n","import pandas as pd\n","import csv\n","from glob import glob\n","from scipy import signal\n","from scipy import ndimage\n","from skimage import io\n","from sklearn.linear_model import LinearRegression\n","from skimage.util import img_as_uint\n","import matplotlib as mpl\n","from skimage.metrics import structural_similarity\n","from skimage.metrics import peak_signal_noise_ratio as psnr\n","from astropy.visualization import simple_norm\n","from skimage import img_as_float32\n","from skimage.util import img_as_ubyte\n","from tqdm import tqdm \n","\n","class bcolors:\n","  WARNING = '\\033[31m'\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","metadata":{"id":"5LEowmfAWqPs"},"source":["# **3. Data augmentation**\n","---\n","<font size = 4>\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"OsIBK-sywkfy"},"outputs":[],"source":["#Data augmentation\n","\n","Training_source = \"/home/meyer/code-project/HistoQuant/HistoQuant/nuc_seg/train/source\" \n","Matching_Training_target = True \n","Training_target = \"/home/meyer/code-project/HistoQuant/HistoQuant/nuc_seg/train/mask\" \n","Random_Crop = False \n","Crop_size = 1024  \n","\n","####Choose a factor by which you want to multiply your original dataset\n","\n","Multiply_dataset_by = 4 \n","Saving_path = \"/home/meyer/code-project/HistoQuant/HistoQuant/nuc_seg/train_aug\" \n","\n","\n","###If not, please choose the probability of the following image manipulations to be used to augment your dataset (1 = always used; 0 = disabled ):\n","\n","####Mirror and rotate images\n","rotate_90_degrees = 0.5 \n","rotate_270_degrees = 0.5 \n","flip_left_right = 0.5 \n","flip_top_bottom = 0.5 \n","\n","####Random image Zoom\n","\n","random_zoom = 0 \n","random_zoom_magnification = 0 \n","\n","####Random image distortion\n","\n","random_distortion = 0 \n","\n","####Image shearing and skewing  \n","\n","image_shear = 0 \n","max_image_shear = 1 \n","skew_image = 0 \n","skew_image_magnitude = 0 \n","\n","\n","list_files = os.listdir(Training_source)\n","Nb_files = len(list_files)\n","\n","Nb_augmented_files = (Nb_files * Multiply_dataset_by)\n","\n","\n","Augmented_folder =  Saving_path+\"/Augmented_Folder\"\n","if os.path.exists(Augmented_folder):\n","  shutil.rmtree(Augmented_folder)\n","os.makedirs(Augmented_folder)\n","\n","  \n","Training_source_augmented = Saving_path+\"/source\"\n","\n","if os.path.exists(Training_source_augmented):\n","  shutil.rmtree(Training_source_augmented)\n","os.makedirs(Training_source_augmented)\n","\n","if Matching_Training_target:\n","  Training_target_augmented = Saving_path+\"/mask\"\n","\n","  if os.path.exists(Training_target_augmented):\n","    shutil.rmtree(Training_target_augmented)\n","  os.makedirs(Training_target_augmented)\n","\n","\n","# Here we generate the augmented images\n","#Load the images\n","p = Augmentor.Pipeline(Training_source, Augmented_folder)\n","\n","#Define the matching images\n","if Matching_Training_target:\n","  p.ground_truth(Training_target)\n","#Define the augmentation possibilities\n","\n","\n","if Random_Crop:\n","  p.crop_by_size(probability=1, width=Crop_size, height=Crop_size, centre=False)\n","\n","if not rotate_90_degrees == 0:\n","  p.rotate90(probability=rotate_90_degrees)\n","  \n","if not rotate_270_degrees == 0:\n","  p.rotate270(probability=rotate_270_degrees)\n","\n","if not flip_left_right == 0:\n","  p.flip_left_right(probability=flip_left_right)\n","\n","if not flip_top_bottom == 0:\n","  p.flip_top_bottom(probability=flip_top_bottom)\n","\n","if not random_zoom == 0:\n","  p.zoom_random(probability=random_zoom, percentage_area=random_zoom_magnification)\n"," \n","if not random_distortion == 0:\n","  p.random_distortion(probability=random_distortion, grid_width=4, grid_height=4, magnitude=8)\n","\n","if not image_shear == 0:\n","  p.shear(probability=image_shear,max_shear_left=20,max_shear_right=20)\n","  \n","if not skew_image == 0:\n","  p.skew(probability=skew_image,magnitude=skew_image_magnitude)\n","\n","p.sample(int(Nb_augmented_files))\n","\n","print(int(Nb_augmented_files),\"images generated\")\n","\n","# Here we sort through the images and move them back to augmented trainning source and targets folders\n","\n","augmented_files = os.listdir(Augmented_folder)\n","\n","for f in augmented_files:\n","\n","  if (f.startswith(\"_groundtruth_(1)_\")):\n","    shortname_noprefix = f[17:]\n","    shutil.copyfile(Augmented_folder+\"/\"+f, Training_target_augmented+\"/\"+shortname_noprefix) \n","  if not (f.startswith(\"_groundtruth_(1)_\")):\n","    shutil.copyfile(Augmented_folder+\"/\"+f, Training_source_augmented+\"/\"+f)\n","      \n","\n","for filename in os.listdir(Training_source_augmented):\n","  os.chdir(Training_source_augmented)\n","  os.rename(filename, filename.replace('_original', ''))\n","  \n","  #Here we clean up the extra files\n","shutil.rmtree(Augmented_folder)\n"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Copie de Augmentor_ZeroCostDL4Mic.ipynb","provenance":[{"file_id":"https://github.com/HenriquesLab/ZeroCostDL4Mic/blob/master/Tools/Augmentor_ZeroCostDL4Mic.ipynb","timestamp":1651653330118},{"file_id":"1GGcClcYdjPIwSQEE4LPi9G6Yr7Q1z_6c","timestamp":1592382350044},{"file_id":"1mqcexfPBaIWuvMWWbJZUFtPoZoJJwrEA","timestamp":1589278334507},{"file_id":"159ARwlQE7-zi0EHxunOF_YPFLt-ZVU5x","timestamp":1587562499898},{"file_id":"1W-7NHehG5MRFILvZZzhPWWnOdJMkadb2","timestamp":1586332290412},{"file_id":"1pUetEQICxYWkYVaQIgdRH1EZBTl7oc2A","timestamp":1586292199692},{"file_id":"1MD36ZkM6XR9EuV12zimJmfCjzyeYZFWq","timestamp":1586269469061},{"file_id":"16A2mbaHzlEElntS8qkFBOsBvZG-mUeY6","timestamp":1586253795726},{"file_id":"1gJlcjOiSxr2buDOxmcFbT_d-GqwLjXtK","timestamp":1583343225796},{"file_id":"10yGI51WzHfgWgZAyE-EbkZFEvIOd6CP6","timestamp":1583171396283}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}
